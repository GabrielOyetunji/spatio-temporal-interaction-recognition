# ðŸ¤¸ Spatio-Temporal Interaction Recognition with ST-GCN

[![Open SBU Notebook](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GabrielOyetunji/spatio-temporal-interaction-recognition/blob/main/Final__SBU_dataset.ipynb)
[![Open UTI Notebook](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GabrielOyetunji/spatio-temporal-interaction-recognition/blob/main/Final__UTI_dataset.ipynb)

Skeleton-based Human Interaction Understanding using Enhanced ST-GCN Architectures
...
ðŸ“Œ Spatio-Temporal Interaction Recognition with ST-GCCN & Hybrid Models

Skeleton-based Human Interaction Understanding using Enhanced ST-GCN Architectures

This repository implements and evaluates multiple Spatio-Temporal Graph Convolutional Network (ST-GCN) variants for recognising two-person human interactions using skeleton-sequence data.
The project explores how architectural enhancements â€” Multi-Stream Fusion, Dilated TCN, SE-Attention, and Transformer-based Temporal Reasoning â€” affect performance across structured and unstructured datasets.

The work is based on the original ST-GCN (Yan et al., 2018) and extends it with eight experimental variants.

â¸»

ðŸš€ Project Overview

Skeleton-based interaction recognition provides a privacy-preserving, noise-robust alternative to RGB-based methods.
However, existing models struggle with:
	â€¢	Changing viewpoints
	â€¢	Occlusions
	â€¢	Non-uniform motion
	â€¢	Unconstrained (real-world-like) recording conditions

This repo evaluates nine distinct architectures across:
	â€¢	SBU Kinect Interaction dataset (structured, clean skeletons)
	â€¢	UT Interaction dataset (unconstrained, noisy, variable sequence lengths)

â¸»

ðŸ§  Models Implemented

Baseline
	â€¢	Model 1 â€“ Original ST-GCN (AAAI 2018)

Single-stream Enhancements
	â€¢	Model 2 â€“ Multi-Stream ST-GCN
	â€¢	Model 3 â€“ Dilated-TCN ST-GCN
	â€¢	Model 4 â€“ SE-Attention ST-GCN
	â€¢	Model 5 â€“ Transformer Temporal Head

Multi-stream Hybrids
	â€¢	Model 6 â€“ Multi-Stream + Dilated TCN
	â€¢	Model 7 â€“ Multi-Stream + SE Attention
	â€¢	Model 8 â€“ Multi-Stream + Transformer
	â€¢	Model 9 â€“ Full Hybrid (All Modules Combined)

â¸»

ðŸ“Š Key Results

SBU Kinect Interaction (Structured Dataset)
	â€¢	Full Hybrid (Model 9) achieved the highest performance
	â€¢	Mean Accuracy: 0.981
	â€¢	Mean F1-Score: 0.983
	â€¢	Transformer-enhanced and dilated-TCN models also performed strongly.

UT Interaction (Unconstrained Dataset)
	â€¢	Performance dropped significantly across all models due to noise, occlusion, and viewpoint changes.
	â€¢	Model 8 (Multi-Stream Transformer) achieved the best F1-Score: ~0.459
	â€¢	Shows that temporal reasoning generalises better than pure spatial refinements.